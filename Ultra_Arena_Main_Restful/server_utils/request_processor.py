#!/usr/bin/env python3
"""
Request Processing Module for Ultra Arena RESTful API

This module handles all request processing operations using the optimized configuration system.
"""

import logging
import time
from pathlib import Path
from typing import Dict, Any, Tuple
from flask import jsonify

from .config_manager import ConfigManager
from .request_validator import RequestValidator
from .config_assemblers import ConfigAssemblyResult

logger = logging.getLogger(__name__)


class RequestProcessor:
    """Processes requests for the Ultra Arena RESTful API using optimized configuration."""
    
    def __init__(self, config_manager: ConfigManager):
        """
        Initialize the request processor.
        
        Args:
            config_manager: The configuration manager instance
        """
        self.config_manager = config_manager
    
    def validate_combo_request(self, data: Dict[str, Any]) -> Tuple[bool, str]:
        """
        Validate combo processing request before configuration assembly.
        
        Args:
            data: The request data to validate
            
        Returns:
            Tuple[bool, str]: (is_valid, error_message)
        """
        # Validate required fields
        required_fields = ["combo_name", "input_pdf_dir_path", "output_dir"]
        is_valid, error_msg = RequestValidator.validate_required_fields(data, required_fields)
        if not is_valid:
            return False, error_msg
        
        # Validate combo name exists
        available_combos = self.config_manager.get_available_combos()
        is_valid, error_msg = RequestValidator.validate_combo_name(data['combo_name'], available_combos)
        if not is_valid:
            return False, error_msg
        
        # Validate file paths
        is_valid, error_msg = RequestValidator.validate_file_paths(data)
        if not is_valid:
            return False, error_msg
        
        # Validate evaluation mode requirements
        if data.get('run_type') == 'evaluation':
            if not data.get('benchmark_file_path'):
                return False, "benchmark_file_path is required when run_type='evaluation'"
        
        return True, None
    
    def create_unified_request_config(self, data: Dict[str, Any], use_default_combo: bool = False) -> Dict[str, Any]:
        """
        Create unified configuration for both simple and combo processing requests.
        
        Args:
            data: Request data from the endpoint
            use_default_combo: If True, use DEFAULT_STRATEGY_PARAM_GRP from profile config and force max_cc_strategies=1
            
        Returns:
            Dict[str, Any]: Unified configuration
            
        Raises:
            ValueError: If configuration creation failed
        """
        try:
            # Validate request before processing (only for combo requests)
            if not use_default_combo:
                is_valid, error_msg = self.validate_combo_request(data)
                if not is_valid:
                    raise ValueError(error_msg)
            
            # Generate request metadata for tracking
            try:
                from Ultra_Arena_Main.common.request_id_generator import RequestIDGenerator
                request_metadata = RequestIDGenerator.create_request_metadata()
                logger.info(f"üîë Generated request metadata: {request_metadata['request_id']}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not generate request metadata: {e}")
                request_metadata = {
                    "request_id": "unknown",
                    "request_mechanism": "rest",
                    "request_start_time": "",
                    "utc_timezone": "UTC"
                }
            
            # Use the optimized configuration assembler
            config_result = self.config_manager.assemble_request_config(data, use_default_combo)
            
            # Log configuration summary
            self.config_manager.get_request_assembler().log_configuration_summary(config_result)
            
            # Inject final configuration into config_base for backward compatibility
            self.config_manager.get_request_assembler().inject_final_config_into_base(config_result)
            
            # Convert to the expected format for backward compatibility
            request_config = config_result.request_config
            
            return {
                "combo_name": request_config.combo_name,
                "input_pdf_dir_path": request_config.input_pdf_dir_path,
                "benchmark_eval_mode": request_config.final_processing.benchmark_eval_mode,
                "streaming": request_config.final_processing.streaming,
                "max_cc_strategies": request_config.final_processing.max_cc_strategies,
                "max_cc_filegroups": request_config.final_processing.max_cc_filegroups,
                "max_files_per_request": request_config.final_processing.max_files_per_request,
                "output_dir": request_config.output_dir,
                "benchmark_file_path": request_config.benchmark_file_path,
                "request_metadata": request_metadata,  # Add request metadata
                "prompt_config": {
                    "system_prompt": request_config.final_prompts.system_prompt,
                    "user_prompt": request_config.final_prompts.user_prompt,
                    "json_format_instructions": request_config.final_prompts.json_format_instructions,
                    "mandatory_keys": request_config.final_prompts.mandatory_keys,
                    "text_first_regex_criteria": request_config.final_prompts.text_first_regex_criteria,
                    "_source_info": request_config.final_prompts.source_info
                },
                "run_config": {
                    "input_pdf_dir_path": str(request_config.input_pdf_dir_path) if request_config.input_pdf_dir_path else None,
                    "output_dir": str(request_config.output_dir) if request_config.output_dir else None,
                    "benchmark_file_path": str(request_config.benchmark_file_path) if request_config.benchmark_file_path else None,
                    "benchmark_eval_mode": request_config.final_processing.benchmark_eval_mode,
                    "pdf_file_paths": []  # Will be populated during processing
                },
                "_config_result": config_result  # Store the full result for response formatting
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error in create_unified_request_config: {e}")
            raise ValueError(f"Configuration creation failed: {str(e)}")
    
    def execute_processing(self, config: Dict[str, Any]) -> int:
        """
        Execute processing with the given configuration.
        
        Args:
            config: The processing configuration
            
        Returns:
            int: Result code from processing
        """
        processing_start_time = time.time()
        
        # Time profile configuration injection (now very fast due to caching)
        logger.info(f"‚è±Ô∏è  START: Profile Configuration Injection")
        profile_injection_start = time.time()
        
        # Configuration is already injected by the assembler, just verify
        self.config_manager.inject_profile_config()
        
        profile_injection_time = time.time() - profile_injection_start
        logger.info(f"‚è±Ô∏è  END: Profile Configuration Injection - Duration: {profile_injection_time:.3f}s")
        
        # Time module import
        import_start_time = time.time()
        from main_modular import run_combo_processing
        import_time = time.time() - import_start_time
        logger.info(f"‚è±Ô∏è  Module Import Time: {import_time:.3f}s")
        
        # Time the main processing call (this is the core bottleneck)
        main_processing_start = time.time()
        logger.info(f"‚è±Ô∏è  START: Main Library Processing Call")
        
        result_code = run_combo_processing(
            combo_name=config["combo_name"],
            benchmark_eval_mode=config["benchmark_eval_mode"],
            streaming=config["streaming"],
            max_cc_strategies=config["max_cc_strategies"],
            max_cc_filegroups=config["max_cc_filegroups"],
            max_files_per_request=config["max_files_per_request"],
            input_pdf_dir_path=config["input_pdf_dir_path"],
            pdf_file_paths=[],
            output_dir=config["output_dir"],
            benchmark_file_path=config["benchmark_file_path"]
        )
        
        main_processing_time = time.time() - main_processing_start
        logger.info(f"‚è±Ô∏è  END: Main Library Processing Call - Duration: {main_processing_time:.3f}s")
        
        # Log detailed breakdown
        total_processing_time = time.time() - processing_start_time
        logger.info(f"üìä MAIN PROCESSING BREAKDOWN:")
        logger.info(f"  Profile Injection: {profile_injection_time:.3f}s")
        logger.info(f"  Module Import: {import_time:.3f}s")
        logger.info(f"  Main Library Call: {main_processing_time:.3f}s")
        logger.info(f"  Total Processing: {total_processing_time:.3f}s")
        
        return result_code
    
    def format_combo_response(self, result_code: int, config: Dict[str, Any]):
        """
        Format response for combo processing endpoint.
        
        Args:
            result_code: The processing result code
            config: The processing configuration
            
        Returns:
            Flask response: Formatted JSON response
        """
        run_config = config.get("run_config", {})
        
        # Determine combo_name for response
        combo_name = config.get("combo_name")
        if combo_name is None:
            combo_name = "default_single_combo_name"
        
        # Include prompt configuration that was used with source information
        prompt_info = {}
        prompt_config = config.get("prompt_config", {})
        
        # Get source information if available
        source_info = prompt_config.get("_source_info", {})
        
        if source_info:
            # Create detailed prompt info with sources
            prompt_details = {}
            for field, info in source_info.items():
                if field != "_source_info":
                    prompt_details[field] = {
                        "value": info.get("value"),
                        "source": info.get("source")
                    }
            
            prompt_info = {
                "prompt_configuration": prompt_details,
                "summary": {
                    "total_prompts": len(prompt_details),
                    "sources_used": list(set(info.get("source") for info in source_info.values() if info.get("source")))
                }
            }
            logger.info(f"üì§ RESPONSE: Including {len(prompt_details)} prompt configurations with sources in response")
        elif prompt_config and len(prompt_config) > 0:
            # Fallback for legacy format
            prompt_info = {
                "prompt_overrides": list(prompt_config.keys()),
                "note": "Legacy format - source information not available"
            }
            logger.info(f"üì§ RESPONSE: Including {len(prompt_config)} prompt overrides in response (legacy)")
        else:
            prompt_info = {
                "prompt_overrides": [],
                "note": "Using profile default prompts"
            }
            logger.info(f"üì§ RESPONSE: Using profile default prompts in response")
        
        # Include configuration assembly performance metrics if available
        config_result = config.get("_config_result")
        performance_info = {}
        if config_result:
            performance_info = {
                "configuration_assembly_time_ms": config_result.assembly_time_ms,
                "server_config_cached": config_result.cache_hit
            }
        
        # Get strategy groups information
        strategy_groups = []
        try:
            from config.config_combo_run import combo_config
            if combo_name and combo_name in combo_config:
                strategy_groups = combo_config[combo_name].get("strategy_groups", [])
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get strategy groups: {e}")
        
        # Get request metadata if available
        request_metadata = config.get("request_metadata", {})
        
        response_data = {
            "status": "success",
            "combo_name": combo_name,
            "benchmark_eval_mode": run_config.get("benchmark_eval_mode", False),
            "input_pdf_dir_path": str(run_config.get('input_pdf_dir_path', '')),
            "output_dir": str(run_config.get('output_dir', '')),
            "benchmark_file_path": str(run_config.get('benchmark_file_path')) if run_config.get('benchmark_file_path') else None,
            "prompt": prompt_info,
            "performance": performance_info,
            "results": result_code,
            # New fields as requested
            "request_id": request_metadata.get("request_id", "unknown"),
            "request_mechanism": request_metadata.get("request_mechanism", "rest"),
            "request_start_time": request_metadata.get("request_start_time", ""),
            "utc_timezone": request_metadata.get("utc_timezone", "UTC"),
            "num_files_to_process": len(run_config.get('pdf_file_paths', [])),
            "num_strategies": len(strategy_groups),
            "strategy_groups": strategy_groups
        }
        
        return jsonify(response_data)
    
    def format_async_combo_response(self, request_id: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Format response for async combo processing endpoint (202 Accepted).
        
        Args:
            request_id: The request ID for tracking
            config: The processing configuration
            
        Returns:
            Dict[str, Any]: Formatted JSON response for async processing
        """
        run_config = config.get("run_config", {})
        
        # Determine combo_name for response
        combo_name = config.get("combo_name")
        if combo_name is None:
            combo_name = "default_single_combo_name"
        
        # Include prompt configuration that was used with source information
        prompt_info = {}
        prompt_config = config.get("prompt_config", {})
        
        # Get source information if available
        source_info = prompt_config.get("_source_info", {})
        
        if source_info:
            # Create detailed prompt info with sources
            prompt_details = {}
            for field, info in source_info.items():
                if field != "_source_info":
                    prompt_details[field] = {
                        "value": info.get("value"),
                        "source": info.get("source")
                    }
            
            prompt_info = {
                "prompt_configuration": prompt_details,
                "summary": {
                    "total_prompts": len(prompt_details),
                    "sources_used": list(set(info.get("source") for info in source_info.values() if info.get("source")))
                }
            }
        elif prompt_config and len(prompt_config) > 0:
            # Fallback for legacy format
            prompt_info = {
                "prompt_overrides": list(prompt_config.keys()),
                "note": "Legacy format - source information not available"
            }
        else:
            prompt_info = {
                "prompt_overrides": [],
                "note": "Using profile default prompts"
            }
        
        # Get strategy groups information
        strategy_groups = []
        try:
            from config.config_combo_run import combo_config
            if combo_name and combo_name in combo_config:
                strategy_groups = combo_config[combo_name].get("strategy_groups", [])
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get strategy groups: {e}")
        
        # Get request metadata if available
        request_metadata = config.get("request_metadata", {})
        
        response_data = {
            "status": "accepted",
            "request_id": request_id,
            "combo_name": combo_name,
            "benchmark_eval_mode": run_config.get("benchmark_eval_mode", False),
            "input_pdf_dir_path": str(run_config.get('input_pdf_dir_path', '')),
            "output_dir": str(run_config.get('output_dir', '')),
            "benchmark_file_path": str(run_config.get('benchmark_file_path')) if run_config.get('benchmark_file_path') else None,
            "prompt": prompt_info,
            # Note: performance and results are not included in async response
            "request_id": request_metadata.get("request_id", "unknown"),
            "request_mechanism": request_metadata.get("request_mechanism", "rest"),
            "request_start_time": request_metadata.get("request_start_time", ""),
            "utc_timezone": request_metadata.get("utc_timezone", "UTC"),
            "num_files_to_process": len(run_config.get('pdf_file_paths', [])),
            "num_strategies": len(strategy_groups),
            "strategy_groups": strategy_groups,
            "message": "Request accepted for processing. Use GET /api/requests/{request_id} to check status."
        }
        
        return response_data
    
    def log_processing_info(self, config: Dict[str, Any], endpoint_name: str):
        """
        Log processing information for both endpoints.
        
        Args:
            config: The processing configuration
            endpoint_name: The name of the endpoint being processed
        """
        combo_name = config.get("combo_name") or "default_single_strategy"
        input_path = config.get("input_pdf_dir_path")
        output_dir = config.get("output_dir")
        benchmark_eval_mode = config.get("benchmark_eval_mode", False)
        benchmark_file_path = config.get("benchmark_file_path")
        
        logger.info(f"Processing {endpoint_name}: {combo_name}")
        logger.info(f"Input: {input_path}")
        logger.info(f"Output: {output_dir}")
        logger.info(f"Benchmark eval mode: {benchmark_eval_mode}")
        if benchmark_file_path:
            logger.info(f"Benchmark file: {benchmark_file_path}")
